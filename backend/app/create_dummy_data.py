"""
ì¶”ì²œ API í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ë”ë¯¸ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸
"""
from .database import SessionLocal
from .models import User, Paper, Recommendation, PaperMetadata, CitationGraph
from datetime import datetime, date, timedelta
import json
import bcrypt

def create_dummy_data():
    db = SessionLocal()
    
    try:
        # 1. í…ŒìŠ¤íŠ¸ìš© ì‚¬ìš©ì ìƒì„± (ì—†ëŠ” ê²½ìš°ì—ë§Œ)
        test_user = db.query(User).filter(User.username == "testuser").first()
        if not test_user:
            hashed_password = bcrypt.hashpw("testpass123".encode('utf-8'), bcrypt.gensalt()).decode('utf-8')
            test_user = User(
                username="testuser",
                password=hashed_password,
                interest="RAG",
                level="intermediate"
            )
            db.add(test_user)
            db.flush()
            print(f"âœ… ì‚¬ìš©ì ìƒì„±: user_id={test_user.user_id}, username={test_user.username}")
        else:
            print(f"âœ… ê¸°ì¡´ ì‚¬ìš©ì ì‚¬ìš©: user_id={test_user.user_id}, username={test_user.username}")
        
        # 2. ë”ë¯¸ ë…¼ë¬¸ ìƒì„±
        papers_data = [
            {
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"],
                "published_date": "2018-10-11",
                "source": "arXiv",
                "external_id": "1810.04805",
                "pdf_url": "https://arxiv.org/pdf/1810.04805.pdf",
                "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers."
            },
            {
                "title": "Dense Passage Retrieval for Open-Domain Question Answering",
                "authors": ["Kaitao Song", "Xu Tan", "Tao Qin", "Jianfeng Lu", "Tie-Yan Liu"],
                "published_date": "2020-10-01",
                "source": "arXiv",
                "external_id": "2004.04906",
                "pdf_url": "https://arxiv.org/pdf/2004.04906.pdf",
                "abstract": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts."
            },
            {
                "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin"],
                "published_date": "2020-05-22",
                "source": "arXiv",
                "external_id": "2005.11401",
                "pdf_url": "https://arxiv.org/pdf/2005.11401.pdf",
                "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters."
            },
            {
                "title": "Attention Is All You Need",
                "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones"],
                "published_date": "2017-06-12",
                "source": "arXiv",
                "external_id": "1706.03762",
                "pdf_url": "https://arxiv.org/pdf/1706.03762.pdf",
                "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks."
            },
            {
                "title": "GPT-3: Language Models are Few-Shot Learners",
                "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan"],
                "published_date": "2020-05-28",
                "source": "arXiv",
                "external_id": "2005.14165",
                "pdf_url": "https://arxiv.org/pdf/2005.14165.pdf",
                "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text."
            }
        ]
        
        created_papers = []
        today = date.today()
        
        for paper_data in papers_data:
            # ë…¼ë¬¸ì´ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸ (titleë¡œ)
            existing_paper = db.query(Paper).filter(Paper.title == paper_data["title"]).first()
            if existing_paper:
                created_papers.append(existing_paper)
                print(f"âœ… ê¸°ì¡´ ë…¼ë¬¸ ì‚¬ìš©: paper_id={existing_paper.paper_id}, title={existing_paper.title[:50]}...")
                continue
            
            # authorsë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
            authors_json = json.dumps(paper_data["authors"])
            
            # external_idì— arXiv: ì ‘ë‘ì‚¬ ì¶”ê°€
            external_id = paper_data.get("external_id", "")
            if external_id and not external_id.startswith("arXiv:"):
                external_id = f"arXiv:{external_id}"
            
            paper = Paper(
                title=paper_data["title"],
                authors=authors_json,
                published_date=paper_data["published_date"],
                source=paper_data["source"],
                external_id=external_id,
                pdf_url=paper_data["pdf_url"],
                abstract=paper_data["abstract"]
            )
            db.add(paper)
            db.flush()
            created_papers.append(paper)
            print(f"âœ… ë…¼ë¬¸ ìƒì„±: paper_id={paper.paper_id}, title={paper.title[:50]}...")
        
        db.commit()
        
        # 3. ì˜¤ëŠ˜ ë‚ ì§œì˜ ì¶”ì²œ ë…¼ë¬¸ ìƒì„± (3ê°œ - relations API í…ŒìŠ¤íŠ¸ìš©)
        today_start = datetime.combine(today, datetime.min.time())
        
        # ê¸°ì¡´ ì˜¤ëŠ˜ ì¶”ì²œì´ ìˆëŠ”ì§€ í™•ì¸
        existing_today_recs = db.query(Recommendation).filter(
            Recommendation.user_id == test_user.user_id,
            Recommendation.recommended_at >= today_start,
            Recommendation.recommended_at < today_start + timedelta(days=1)
        ).all()
        
        recommended_papers = []
        if existing_today_recs:
            print(f"âœ… ê¸°ì¡´ ì˜¤ëŠ˜ ì¶”ì²œ ë…¼ë¬¸ {len(existing_today_recs)}ê°œ ë°œê²¬")
            recommended_papers = [rec.paper for rec in existing_today_recs if rec.paper]
        else:
            # ì˜¤ëŠ˜ ì¶”ì²œ ë…¼ë¬¸ 3ê°œ ìƒì„± (relations API í…ŒìŠ¤íŠ¸ìš©)
            for i, paper in enumerate(created_papers[:3]):
                rec_time = today_start + timedelta(hours=i*2)  # ì‹œê°„ ê°„ê²©ì„ ë‘ê³ 
                recommendation = Recommendation(
                    user_id=test_user.user_id,
                    paper_id=paper.paper_id,
                    recommended_at=rec_time,
                    is_user_requested=False
                )
                db.add(recommendation)
                recommended_papers.append(paper)
                print(f"âœ… ì˜¤ëŠ˜ ì¶”ì²œ ë…¼ë¬¸ ìƒì„±: paper_id={paper.paper_id}, recommended_at={rec_time}")
            
            db.commit()
        
        # 4. ê³µí†µ ì¸ìš© ë…¼ë¬¸ ìƒì„± (Attention Is All You Need - relations API í…ŒìŠ¤íŠ¸ìš©)
        common_ref_paper = None
        if len(created_papers) >= 4:
            common_ref_paper = created_papers[3]  # Attention Is All You Need
        else:
            # ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            common_ref_data = {
                "title": "Attention Is All You Need",
                "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones"],
                "published_date": "2017-06-12",
                "source": "arXiv",
                "external_id": "arXiv:1706.03762",
                "pdf_url": "https://arxiv.org/pdf/1706.03762.pdf",
                "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks."
            }
            
            existing_common = db.query(Paper).filter(
                Paper.external_id == "arXiv:1706.03762"
            ).first()
            
            if existing_common:
                common_ref_paper = existing_common
                print(f"âœ… ê¸°ì¡´ ê³µí†µ ì¸ìš© ë…¼ë¬¸ ì‚¬ìš©: paper_id={common_ref_paper.paper_id}")
            else:
                authors_json = json.dumps(common_ref_data["authors"])
                common_ref_paper = Paper(
                    title=common_ref_data["title"],
                    authors=authors_json,
                    published_date=common_ref_data["published_date"],
                    source=common_ref_data["source"],
                    external_id=common_ref_data["external_id"],
                    pdf_url=common_ref_data["pdf_url"],
                    abstract=common_ref_data["abstract"]
                )
                db.add(common_ref_paper)
                db.flush()
                print(f"âœ… ê³µí†µ ì¸ìš© ë…¼ë¬¸ ìƒì„±: paper_id={common_ref_paper.paper_id}")
                db.commit()
        
        # 5. ì¸ìš© ê´€ê³„ ìƒì„± (3ê°œ ì¶”ì²œ ë…¼ë¬¸ì´ ëª¨ë‘ ê³µí†µ ì¸ìš© ë…¼ë¬¸ì„ ì¸ìš©)
        if common_ref_paper and len(recommended_papers) >= 3:
            for rec_paper in recommended_papers[:3]:
                # ê¸°ì¡´ ì¸ìš© ê´€ê³„ í™•ì¸
                existing_citation = db.query(CitationGraph).filter(
                    CitationGraph.citing_paper_id == rec_paper.paper_id,
                    CitationGraph.cited_paper_id == common_ref_paper.paper_id
                ).first()
                
                if existing_citation:
                    print(f"âœ… ê¸°ì¡´ ì¸ìš© ê´€ê³„ ì‚¬ìš©: {rec_paper.paper_id} -> {common_ref_paper.paper_id}")
                    continue
                
                # ì¸ìš© ê´€ê³„ ìƒì„± (ì²« ë²ˆì§¸ëŠ” influential, ë‚˜ë¨¸ì§€ëŠ” ì¼ë°˜)
                is_influential = 1 if rec_paper == recommended_papers[0] else 0
                citation = CitationGraph(
                    citing_paper_id=rec_paper.paper_id,
                    cited_paper_id=common_ref_paper.paper_id,
                    relation_type="cites",
                    is_influential=is_influential
                )
                db.add(citation)
                print(f"âœ… ì¸ìš© ê´€ê³„ ìƒì„±: paper_id={rec_paper.paper_id} -> {common_ref_paper.paper_id} (influential={is_influential})")
            
            db.commit()
        
        # 6. ì¼ë¶€ ë…¼ë¬¸ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for i, paper in enumerate(created_papers[:2]):
            existing_metadata = db.query(PaperMetadata).filter(
                PaperMetadata.paper_id == paper.paper_id
            ).first()
            
            if existing_metadata:
                print(f"âœ… ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì‚¬ìš©: paper_id={paper.paper_id}")
                continue
            
            keywords_json = json.dumps(["RAG", "Retrieval", "Knowledge-Intensive NLP", "Language Models"])
            metadata = PaperMetadata(
                paper_id=paper.paper_id,
                summary_level="intermediate",
                summary_content="ì´ ë…¼ë¬¸ì€ ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸ì— ì™¸ë¶€ ì§€ì‹ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ê²°í•©í•œ Retrieval-Augmented Generation (RAG) ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.",
                keywords=keywords_json,
                citation_count=1523,
                citation_velocity=45.2,
                influential_citation_count=234
            )
            db.add(metadata)
            print(f"âœ… ë©”íƒ€ë°ì´í„° ìƒì„±: paper_id={paper.paper_id}")
        
        db.commit()
        
        print("\n" + "="*60)
        print("âœ… ë”ë¯¸ ë°ì´í„° ìƒì„± ì™„ë£Œ!")
        print("="*60)
        print(f"\nğŸ“Œ í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì ì •ë³´:")
        print(f"   user_id: {test_user.user_id}")
        print(f"   username: {test_user.username}")
        print(f"\nğŸ“Œ ìƒì„±ëœ ë…¼ë¬¸ ìˆ˜: {len(created_papers)}ê°œ")
        if common_ref_paper:
            print(f"   ê³µí†µ ì¸ìš© ë…¼ë¬¸: paper_id={common_ref_paper.paper_id}, title={common_ref_paper.title[:50]}...")
        print(f"\nğŸ“Œ ì˜¤ëŠ˜ ì¶”ì²œ ë…¼ë¬¸: {len(recommended_papers)}ê°œ")
        for i, paper in enumerate(recommended_papers[:3], 1):
            print(f"   {i}. paper_id={paper.paper_id}, title={paper.title[:50]}...")
        print(f"\nğŸ“Œ í…ŒìŠ¤íŠ¸í•  API:")
        print(f"   1. GET /api/v1/{test_user.user_id}/recommendations/today")
        print(f"      â†’ ì˜¤ëŠ˜ì˜ ì¶”ì²œ ë…¼ë¬¸ ì¡°íšŒ")
        print(f"   2. GET /api/v1/{test_user.user_id}/recommendations/today/relations")
        print(f"      â†’ ì˜¤ëŠ˜ì˜ ì¶”ì²œ ë…¼ë¬¸ ì¸ìš© ê´€ê³„ ë¶„ì„")
        if common_ref_paper:
            print(f"   3. POST /api/v1/{test_user.user_id}/recommendations/request-paper")
            print(f"      â†’ ê³µí†µ ì°¸ê³ ë¬¸í—Œ ì¶”ì²œ ìˆ˜ë½")
            print(f"      (body: {{'paper_id': {common_ref_paper.paper_id}, 'reason': 'common_reference'}})")
        print("\n")
        
    except Exception as e:
        db.rollback()
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
        raise
    finally:
        db.close()


if __name__ == "__main__":
    print("\n=== ì¶”ì²œ API í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„° ìƒì„± ì‹œì‘ ===\n")
    create_dummy_data()
    print("\n=== ì™„ë£Œ ===\n")